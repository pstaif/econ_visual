{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fab7ad",
   "metadata": {},
   "source": [
    "Conceptually: strong idea, and the “mixture of experts” framing maps very naturally to how economists actually work (parse → frame → model → solve → interpret → critique assumptions). The main question is implementation strategy so you get something usable quickly without getting trapped in “train-a-giant-model-from-scratch” territory.\n",
    "\n",
    "1) What I think works best in practice (and why)\n",
    "\n",
    "A. Start as agentic MoE (router + specialist experts), not “true MoE pretraining”\n",
    "\n",
    "“True” MoE (multiple expert subnetworks inside one transformer trained jointly) is powerful but expensive and operationally heavy. You can get 80–90% of the benefit by doing:\n",
    "\t•\tA Router/Framer model that classifies + structures the task\n",
    "\t•\tSpecialist expert models (or LoRA adapters) per subfield/task\n",
    "\t•\tA tool/code expert that writes and executes code\n",
    "\t•\tA critic that checks assumptions, units, equilibrium conditions, identification strategy, etc.\n",
    "\n",
    "This is also far easier to iterate on, evaluate, and swap components.\n",
    "\n",
    "B. Use RAG + structured knowledge as the foundation, fine-tuning second\n",
    "\n",
    "For economics, correctness often depends on definitions, assumptions, and canonical results. RAG over your curated sources plus a knowledge graph of concepts is extremely effective, and it avoids “baking in” errors.\n",
    "\n",
    "Then fine-tune (LoRA/QLoRA) to improve:\n",
    "\t•\tframing templates\n",
    "\t•\tstep ordering\n",
    "\t•\tstyle (econ-precise)\n",
    "\t•\tconsistent notation\n",
    "\t•\tcode patterns\n",
    "\n",
    "C. You should treat “econ ML expert → code” as a first-class toolchain\n",
    "\n",
    "Make code generation verifiable:\n",
    "\t•\tgenerated code runs in a sandbox (Python)\n",
    "\t•\tincludes tests or sanity checks (units, signs, boundary cases)\n",
    "\t•\tproduces plots/tables where relevant\n",
    "\t•\tlogs decisions and assumptions\n",
    "\n",
    "That makes the system materially better than a “smart text bot”.\n",
    "\n",
    "2) Data plan: what to use and what to avoid\n",
    "\n",
    "What’s ideal\n",
    "\t•\tYour own notes, homework solutions, summaries, derivations, cheat sheets\n",
    "\t•\tProblem sets where you have the right to use the material\n",
    "\t•\tPublic-domain / permissively licensed resources\n",
    "\t•\tSelf-generated Q/A pairs from your notes (with careful review)\n",
    "\n",
    "What’s risky\n",
    "\t•\tTextbooks: typically copyrighted. Using them for training is a legal/licensing question.\n",
    "\t•\tQuizlet: content and access are governed by their Terms; scraping or bulk use can be a problem, and sets may be copyrighted by the creators.\n",
    "\n",
    "Pragmatic approach:\n",
    "\t•\tUse copyrighted sources for private RAG if you have lawful access and keep it internal (still not legal advice), but avoid training directly on them unless you have permission.\n",
    "\t•\tPrefer converting what you learn into your own “canonical cards” (definitions, theorems, standard derivations, solved examples) and train on those.\n",
    "\n",
    "3) Proposed expert lineup (a concrete blueprint)\n",
    "\t1.\tProblem Digestor (Econ Parser)\n",
    "\n",
    "\t•\tExtracts: agents, goods, constraints, timing, information, equilibrium concept\n",
    "\t•\tDetects: “this is actually a mechanism design / adverse selection / IV / general equilibrium / game” etc.\n",
    "\t•\tOutput format: a structured schema (JSON)\n",
    "\n",
    "\t2.\tField Router\n",
    "\n",
    "\t•\tMulti-label classification: micro theory, macro, IO, metrics, finance, public, labor, trade, dev, behavioral, etc.\n",
    "\t•\tRoutes to 1–3 specialists, not just one\n",
    "\n",
    "\t3.\tModel Framer\n",
    "\n",
    "\t•\tSelect canonical model class + assumptions\n",
    "\t•\tOffers alternatives (e.g., partial vs general equilibrium; static vs dynamic; OLS vs IV vs DiD)\n",
    "\n",
    "\t4.\tSolver / Derivation Expert\n",
    "\n",
    "\t•\tProduces the math (FOCs, comparative statics, equilibrium existence, welfare)\n",
    "\n",
    "\t5.\tEmpirical Strategy Expert (Metrics)\n",
    "\n",
    "\t•\tIdentification strategy, threats, robustness, DAG thinking\n",
    "\t•\tSuggests data transforms + estimator choice\n",
    "\n",
    "\t6.\tCode Expert\n",
    "\n",
    "\t•\tTurns the framed model/estimation into reproducible code\n",
    "\t•\tAdds checks, diagnostics, and figures\n",
    "\n",
    "\t7.\tCritic / Referee #2\n",
    "\n",
    "\t•\tAssumption audit, sign checks, edge cases\n",
    "\t•\t“What would a journal referee attack?”\n",
    "\n",
    "This is exactly the kind of structure that benefits from a router + specialists.\n",
    "\n",
    "4) Minimum Viable Product I’d build first (fast path)\n",
    "\n",
    "MVP goal: A system that takes any econ question and returns:\n",
    "\t•\tstructured framing (schema)\n",
    "\t•\trecommended approach (model/estimator)\n",
    "\t•\tsolution sketch (math or logic)\n",
    "\t•\toptional runnable code (if asked)\n",
    "\t•\ta short “assumptions & failure modes” section\n",
    "\n",
    "Implementation: start with one strong base LLM + prompt-routed agents + RAG over your notes.\n",
    "\n",
    "Then iterate:\n",
    "\t•\tfine-tune router (cheap)\n",
    "\t•\tfine-tune “framer” and “critic” on your canonical format\n",
    "\t•\tlater: LoRA adapters per subfield\n",
    "\n",
    "5) The hardest parts (so you don’t get blindsided)\n",
    "\t•\tEvaluation: you need a gold set of questions with expected framing and key points. Without this, progress becomes vibes-based.\n",
    "\t•\tNotation consistency: econ is fragile to notation drift; solve with a global style guide + schema + critic enforcement.\n",
    "\t•\tData cleanliness: your notes/homework will be noisy. The trick is converting them into clean, structured “cards” and “worked examples”.\n",
    "\t•\tCopyright/ToS: biggest non-technical risk. Design the pipeline so you can swap in compliant datasets easily.\n",
    "\n",
    "6) If you want my recommendation in one line\n",
    "\n",
    "Build it as an agentic MoE with a strong router + RAG + verifier, then add LoRA specialists once you’ve locked the schema and evaluation suite.\n",
    "\n",
    "If you reply with (a) which subfields you care about most (top 5) and (b) your intended runtime stack (local Ollama vs API vs server GPUs), I’ll propose a concrete architecture diagram + repo structure + the exact schemas/prompts for each expert so you can start implementing immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a05f0c",
   "metadata": {},
   "source": [
    "pip install pymupdf neo4j openai tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from neo4j import GraphDatabase\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEFAULT_EMBED_MODEL = \"text-embedding-3-small\"  # 1536 dims in common usage\n",
    "# OpenAI embeddings guide lists text-embedding-3-small / 3-large as current.   [oai_citation:4‡OpenAI Platform](https://platform.openai.com/docs/guides/embeddings?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "960b4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = re.sub(r\"[ \\t]+\\n\", \"\\n\", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def chunk_text(text: str, target_chars: int = 2500, overlap_chars: int = 250) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simple, robust chunker (char-based) that tries to keep paragraphs together.\n",
    "    Good enough for v0; you can swap to token-based later.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks = []\n",
    "    cur = \"\"\n",
    "\n",
    "    for p in paras:\n",
    "        if len(cur) + len(p) + 2 <= target_chars:\n",
    "            cur = (cur + \"\\n\\n\" + p).strip() if cur else p\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            # if paragraph is huge, hard-split\n",
    "            if len(p) > target_chars:\n",
    "                start = 0\n",
    "                while start < len(p):\n",
    "                    end = min(len(p), start + target_chars)\n",
    "                    chunks.append(p[start:end])\n",
    "                    start = max(end - overlap_chars, end)\n",
    "                cur = \"\"\n",
    "            else:\n",
    "                cur = p\n",
    "\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "\n",
    "    # add overlap between chunks\n",
    "    if overlap_chars > 0 and len(chunks) > 1:\n",
    "        out = []\n",
    "        for i, c in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                out.append(c)\n",
    "            else:\n",
    "                prev = out[-1]\n",
    "                overlap = prev[-overlap_chars:] if len(prev) > overlap_chars else prev\n",
    "                out.append((overlap + \"\\n\\n\" + c).strip())\n",
    "        return out\n",
    "\n",
    "    return chunks\n",
    "\n",
    "@dataclass\n",
    "class TocItem:\n",
    "    level: int\n",
    "    title: str\n",
    "    start_page: int  # 1-based page number (matches PyMuPDF TOC)\n",
    "    end_page: int    # computed\n",
    "\n",
    "def compute_end_pages(toc: List[Tuple[int, str, int]], total_pages: int) -> List[TocItem]:\n",
    "    \"\"\"\n",
    "    toc is list of (level, title, page) with 1-based page numbers\n",
    "    end_page is next item page - 1 at same or higher level, else total_pages\n",
    "    \"\"\"\n",
    "    items = [TocItem(lvl, title.strip(), int(pg), total_pages) for (lvl, title, pg) in toc]\n",
    "    for i in range(len(items)):\n",
    "        this = items[i]\n",
    "        end = total_pages\n",
    "        for j in range(i + 1, len(items)):\n",
    "            nxt = items[j]\n",
    "            if nxt.level <= this.level:\n",
    "                end = max(this.start_page, nxt.start_page - 1)\n",
    "                break\n",
    "        items[i].end_page = end\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b619c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Neo4j writers (NO APOC)\n",
    "# -----------------------------\n",
    "\n",
    "SCHEMA_CYPHER = \"\"\"\n",
    "MERGE (b:Book {book_id: $book_id})\n",
    "SET b.title = $title,\n",
    "    b.authors = $authors,\n",
    "    b.edition = $edition,\n",
    "    b.year = $year,\n",
    "    b.total_pages = $total_pages;\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_PAGES_CYPHER = \"\"\"\n",
    "UNWIND $pages AS row\n",
    "MERGE (p:Page {book_id: $book_id, page_no: row.page_no})\n",
    "SET p.text = row.text,\n",
    "    p.checksum = row.checksum\n",
    "WITH p\n",
    "MATCH (b:Book {book_id: $book_id})\n",
    "MERGE (b)-[:HAS_PAGE]->(p);\n",
    "\"\"\"\n",
    "\n",
    "# Outline ingestion in 3 passes (Neo4j 5 safe; no MATCH inside FOREACH)\n",
    "UPSERT_OUTLINE_NODES_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "MERGE (o:Outline {outline_id: row.outline_id})\n",
    "SET o.kind = row.kind,\n",
    "    o.level = row.level,\n",
    "    o.title = row.title,\n",
    "    o.start_page = row.start_page,\n",
    "    o.end_page = row.end_page;\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_OUTLINE_BOOK_REL_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "WITH row\n",
    "WHERE row.parent_outline_id IS NULL\n",
    "MATCH (b:Book {book_id: $book_id})\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MERGE (b)-[:HAS_OUTLINE]->(o);\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_OUTLINE_PARENT_REL_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "WITH row\n",
    "WHERE row.parent_outline_id IS NOT NULL\n",
    "MATCH (parent:Outline {outline_id: row.parent_outline_id})\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MERGE (parent)-[:HAS_CHILD]->(o);\n",
    "\"\"\"\n",
    "\n",
    "LINK_COVERS_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MATCH (p:Page {book_id: $book_id, page_no: row.page_no})\n",
    "MERGE (o)-[:COVERS]->(p);\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_CHUNKS_CYPHER = \"\"\"\n",
    "UNWIND $chunks AS row\n",
    "MERGE (c:Chunk {chunk_id: row.chunk_id})\n",
    "SET c.book_id = $book_id,\n",
    "    c.page_no = row.page_no,\n",
    "    c.chunk_index = row.chunk_index,\n",
    "    c.text = row.text,\n",
    "    c.checksum = row.checksum\n",
    "WITH c, row\n",
    "MATCH (p:Page {book_id: $book_id, page_no: row.page_no})\n",
    "MERGE (p)-[:HAS_CHUNK]->(c);\n",
    "\"\"\"\n",
    "\n",
    "# Store embeddings as LIST<FLOAT> property (no procedures)\n",
    "SET_EMBEDDINGS_CYPHER = \"\"\"\n",
    "UNWIND $rows AS row\n",
    "MATCH (c:Chunk {chunk_id: row.chunk_id})\n",
    "SET c.embedding = row.embedding\n",
    "RETURN count(*) AS updated;\n",
    "\"\"\"\n",
    "\n",
    "SUPPORTED_BY_CYPHER = \"\"\"\n",
    "MATCH (o:Outline)-[:COVERS]->(:Page)-[:HAS_CHUNK]->(c:Chunk)\n",
    "MERGE (o)-[:SUPPORTED_BY]->(c);\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Deeper layer v2: Outline -> Concept -> Chunk\n",
    "# (Concept nodes are derived from Outline nodes; deterministic IDs)\n",
    "# -----------------------------\n",
    "\n",
    "CONSTRAINTS_V2_CYPHER = \"\"\"\n",
    "CREATE CONSTRAINT concept_concept_id IF NOT EXISTS\n",
    "FOR (c:Concept) REQUIRE c.concept_id IS UNIQUE;\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_CONCEPTS_FROM_OUTLINE_CYPHER = \"\"\"\n",
    "MATCH (b:Book {book_id:$book_id})-[:HAS_OUTLINE]->(o:Outline)\n",
    "MERGE (c:Concept {concept_id: o.outline_id})\n",
    "SET c.book_id = $book_id,\n",
    "    c.name = o.title,\n",
    "    c.aliases = coalesce(c.aliases, []),\n",
    "    c.kind = o.kind,\n",
    "    c.level = o.level,\n",
    "    c.start_page = o.start_page,\n",
    "    c.end_page = o.end_page;\n",
    "\"\"\"\n",
    "\n",
    "LINK_OUTLINE_HAS_CONCEPT_CYPHER = \"\"\"\n",
    "MATCH (b:Book {book_id:$book_id})-[:HAS_OUTLINE]->(o:Outline)\n",
    "MATCH (c:Concept {concept_id: o.outline_id})\n",
    "MERGE (o)-[:HAS_CONCEPT]->(c);\n",
    "\"\"\"\n",
    "\n",
    "LINK_CHUNK_ABOUT_CONCEPT_CYPHER = \"\"\"\n",
    "MATCH (o:Outline)-[:SUPPORTED_BY]->(ch:Chunk)\n",
    "MATCH (c:Concept {concept_id: o.outline_id})\n",
    "MERGE (ch)-[:ABOUT]->(c);\n",
    "\"\"\"\n",
    "\n",
    "def kind_for_level(level: int) -> str:\n",
    "    if level == 1: return \"chapter\"\n",
    "    if level == 2: return \"subchapter\"\n",
    "    if level == 3: return \"topic\"\n",
    "    return \"subtopic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccea383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--pdf\", required=True, help=\"Path to textbook PDF\")\n",
    "    ap.add_argument(\"--book-id\", required=True)\n",
    "    ap.add_argument(\"--title\", required=True)\n",
    "    ap.add_argument(\"--authors\", default=\"\")\n",
    "    ap.add_argument(\"--edition\", default=\"\")\n",
    "    ap.add_argument(\"--year\", default=\"\")\n",
    "    ap.add_argument(\"--neo4j-uri\", default=\"bolt://localhost:7687\")\n",
    "    ap.add_argument(\"--neo4j-user\", default=\"neo4j\")\n",
    "    ap.add_argument(\"--neo4j-pass\", default=\"testpassword\")\n",
    "    ap.add_argument(\"--embed-model\", default=DEFAULT_EMBED_MODEL)\n",
    "    ap.add_argument(\"--embed-batch\", type=int, default=64)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.pdf):\n",
    "        raise FileNotFoundError(args.pdf)\n",
    "\n",
    "    # ---- Read PDF\n",
    "    doc = fitz.open(args.pdf)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    # ---- Extract TOC (best-case: PDF has a real outline)\n",
    "    raw_toc = doc.get_toc(simple=True)  # [(level, title, pageNo), ...]\n",
    "    toc_items: List[TocItem] = []\n",
    "    if raw_toc:\n",
    "        toc_items = compute_end_pages(raw_toc, total_pages)\n",
    "\n",
    "    # ---- Extract pages\n",
    "    pages_payload = []\n",
    "    for i in range(total_pages):\n",
    "        page_no = i + 1\n",
    "        text = clean_text(doc.load_page(i).get_text(\"text\"))\n",
    "        pages_payload.append({\n",
    "            \"page_no\": page_no,\n",
    "            \"text\": text,\n",
    "            \"checksum\": sha1(text),\n",
    "        })\n",
    "\n",
    "    # ---- Connect Neo4j\n",
    "    driver = GraphDatabase.driver(args.neo4j_uri, auth=(args.neo4j_user, args.neo4j_pass))\n",
    "\n",
    "    with driver.session() as s:\n",
    "        s.run(SET_EMBEDDINGS_CYPHER, rows=payload)\n",
    "\n",
    "        # Pages (batch)\n",
    "        s.run(UPSERT_PAGES_CYPHER, book_id=args.book_id, pages=pages_payload)\n",
    "\n",
    "        # Outline + parent-child\n",
    "        if toc_items:\n",
    "            items_payload = []\n",
    "            parent_stack: Dict[int, str] = {}  # level -> outline_id\n",
    "            for idx, it in enumerate(toc_items):\n",
    "                outline_id = f\"{args.book_id}:toc:{idx}:{sha1(it.title)[:10]}\"\n",
    "                parent_id = parent_stack.get(it.level - 1)\n",
    "                parent_stack[it.level] = outline_id\n",
    "                # truncate deeper levels when we go up\n",
    "                for lvl in list(parent_stack.keys()):\n",
    "                    if lvl > it.level:\n",
    "                        del parent_stack[lvl]\n",
    "\n",
    "                items_payload.append({\n",
    "                    \"outline_id\": outline_id,\n",
    "                    \"parent_outline_id\": parent_id,\n",
    "                    \"kind\": kind_for_level(it.level),\n",
    "                    \"level\": it.level,\n",
    "                    \"title\": it.title,\n",
    "                    \"start_page\": it.start_page,\n",
    "                    \"end_page\": it.end_page,\n",
    "                })\n",
    "\n",
    "            s.run(UPSERT_OUTLINE_CYPHER, book_id=args.book_id, items=items_payload)\n",
    "\n",
    "            # Link COVERS -> Page for every page in range\n",
    "            # For v0 we create explicit links to pages (fine for one book).\n",
    "            covers_rows = []\n",
    "            for row in items_payload:\n",
    "                for p in range(row[\"start_page\"], row[\"end_page\"] + 1):\n",
    "                    covers_rows.append({\"outline_id\": row[\"outline_id\"], \"page_no\": p})\n",
    "            # chunk this to avoid huge tx\n",
    "            CHUNK = 2000\n",
    "            for k in range(0, len(covers_rows), CHUNK):\n",
    "                part = covers_rows[k:k+CHUNK]\n",
    "                s.run(LINK_COVERS_CYPHER, book_id=args.book_id, items=part)\n",
    "\n",
    "        # Chunks\n",
    "        chunk_rows = []\n",
    "        for p in pages_payload:\n",
    "            page_no = p[\"page_no\"]\n",
    "            chunks = chunk_text(p[\"text\"])\n",
    "            for j, ct in enumerate(chunks):\n",
    "                chunk_id = f\"{args.book_id}:p{page_no}:c{j}:{sha1(ct)[:10]}\"\n",
    "                chunk_rows.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"page_no\": page_no,\n",
    "                    \"chunk_index\": j,\n",
    "                    \"text\": ct,\n",
    "                    \"checksum\": sha1(ct),\n",
    "                })\n",
    "\n",
    "        # Insert chunks in manageable batches\n",
    "        CHUNK = 1000\n",
    "        for k in range(0, len(chunk_rows), CHUNK):\n",
    "            s.run(UPSERT_CHUNKS_CYPHER, book_id=args.book_id, chunks=chunk_rows[k:k+CHUNK])\n",
    "\n",
    "        # If we have outline, create SUPPORTED_BY edges (Outline -> Chunk)\n",
    "        if toc_items:\n",
    "            s.run(SUPPORTED_BY_CYPHER)\n",
    "\n",
    "    # ---- Embeddings (OpenAI) and write to Neo4j\n",
    "    client = OpenAI()\n",
    "    batch = args.embed_batch\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        resp = client.embeddings.create(model=args.embed_model, input=texts)\n",
    "        return [d.embedding for d in resp.data]\n",
    "\n",
    "    # embed only chunks missing embeddings\n",
    "    with driver.session() as s:\n",
    "        missing = s.run(\n",
    "            \"MATCH (c:Chunk) WHERE c.book_id=$book_id AND c.embedding IS NULL RETURN c.chunk_id AS id, c.text AS text\",\n",
    "            book_id=args.book_id,\n",
    "        ).data()\n",
    "\n",
    "    rows = [{\"chunk_id\": r[\"id\"], \"text\": r[\"text\"]} for r in missing]\n",
    "\n",
    "    for i in range(0, len(rows), batch):\n",
    "        group = rows[i:i+batch]\n",
    "        vectors = embed_texts([g[\"text\"] for g in group])\n",
    "        payload = [{\"chunk_id\": g[\"chunk_id\"], \"embedding\": v} for g, v in zip(group, vectors)]\n",
    "\n",
    "        with driver.session() as s:\n",
    "            try:\n",
    "                s.run(SET_EMBEDDINGS_CYPHER, rows=payload)\n",
    "            except Exception:\n",
    "                # fallback for older Neo4j builds where the procedure may be absent\n",
    "                s.run(SET_EMBEDDINGS_FALLBACK_CYPHER, rows=payload)\n",
    "\n",
    "    driver.close()\n",
    "    doc.close()\n",
    "\n",
    "    print(f\"Done. Ingested book_id={args.book_id}, pages={total_pages}, chunks={len(chunk_rows)}.\")\n",
    "    print(\"Reminder: CREATE VECTOR INDEX uses the embedding dimension you selected.\")\n",
    "    print(\"Vector index query procedure: CALL db.index.vector.queryNodes(...).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Notebook-friendly entrypoint (FIXED, Neo4j 5 safe, no APOC, no procedures)\n",
    "# -----------------------------\n",
    "def run_ingestion(\n",
    "    pdf: str,\n",
    "    book_id: str,\n",
    "    title: str,\n",
    "    authors: str = \"\",\n",
    "    edition: str = \"\",\n",
    "    year: str = \"\",\n",
    "    neo4j_uri: str = \"bolt://localhost:7687\",\n",
    "    neo4j_user: str = \"neo4j\",\n",
    "    neo4j_pass: str = \"testpassword\",\n",
    "    embed_model: str = DEFAULT_EMBED_MODEL,\n",
    "    embed_batch: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Notebook-friendly runner. Mirrors the argparse main() behavior,\n",
    "    but takes explicit parameters.\n",
    "\n",
    "    Assumes these globals already exist in the notebook:\n",
    "      - SCHEMA_CYPHER, UPSERT_PAGES_CYPHER, LINK_COVERS_CYPHER, UPSERT_CHUNKS_CYPHER\n",
    "      - UPSERT_OUTLINE_NODES_CYPHER, UPSERT_OUTLINE_BOOK_REL_CYPHER, UPSERT_OUTLINE_PARENT_REL_CYPHER\n",
    "      - SUPPORTED_BY_CYPHER, SET_EMBEDDINGS_CYPHER\n",
    "      - sha1, clean_text, chunk_text, compute_end_pages, kind_for_level\n",
    "      - TocItem (dataclass), plus imports (os, fitz, GraphDatabase, OpenAI, Dict, List)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from typing import Dict, List\n",
    "\n",
    "    if not os.path.exists(pdf):\n",
    "        raise FileNotFoundError(pdf)\n",
    "\n",
    "    # ---- Read PDF\n",
    "    doc = fitz.open(pdf)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    # ---- Extract TOC (best-case: PDF has a real outline)\n",
    "    raw_toc = doc.get_toc(simple=True)  # [(level, title, pageNo), ...]\n",
    "    toc_items: List[TocItem] = []\n",
    "    if raw_toc:\n",
    "        toc_items = compute_end_pages(raw_toc, total_pages)\n",
    "\n",
    "    # ---- Extract pages\n",
    "    pages_payload = []\n",
    "    for i in range(total_pages):\n",
    "        page_no = i + 1\n",
    "        text = clean_text(doc.load_page(i).get_text(\"text\"))\n",
    "        pages_payload.append(\n",
    "            {\n",
    "                \"page_no\": page_no,\n",
    "                \"text\": text,\n",
    "                \"checksum\": sha1(text),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # ---- Connect Neo4j\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_pass))\n",
    "\n",
    "    # ---- Write Book, Pages, Outline, Chunks\n",
    "    chunk_rows = []  # keep for return value\n",
    "    try:\n",
    "        with driver.session() as s:\n",
    "            # Book node\n",
    "            s.run(\n",
    "                SCHEMA_CYPHER,\n",
    "                book_id=book_id,\n",
    "                title=title,\n",
    "                authors=authors,\n",
    "                edition=edition,\n",
    "                year=year,\n",
    "                total_pages=total_pages,\n",
    "            )\n",
    "\n",
    "            # Pages\n",
    "            s.run(UPSERT_PAGES_CYPHER, book_id=book_id, pages=pages_payload)\n",
    "\n",
    "            # Outline + parent-child (Neo4j 5 safe; no FOREACH-with-MATCH)\n",
    "            items_payload = []\n",
    "            if toc_items:\n",
    "                parent_stack: Dict[int, str] = {}  # level -> outline_id\n",
    "\n",
    "                for idx, it in enumerate(toc_items):\n",
    "                    outline_id = f\"{book_id}:toc:{idx}:{sha1(it.title)[:10]}\"\n",
    "                    parent_id = parent_stack.get(it.level - 1)\n",
    "\n",
    "                    parent_stack[it.level] = outline_id\n",
    "                    # remove deeper levels when moving up\n",
    "                    for lvl in list(parent_stack.keys()):\n",
    "                        if lvl > it.level:\n",
    "                            del parent_stack[lvl]\n",
    "\n",
    "                    items_payload.append(\n",
    "                        {\n",
    "                            \"outline_id\": outline_id,\n",
    "                            \"parent_outline_id\": parent_id,\n",
    "                            \"kind\": kind_for_level(it.level),\n",
    "                            \"level\": it.level,\n",
    "                            \"title\": it.title,\n",
    "                            \"start_page\": it.start_page,\n",
    "                            \"end_page\": it.end_page,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # 1) nodes\n",
    "                s.run(UPSERT_OUTLINE_NODES_CYPHER, items=items_payload)\n",
    "                # 2) book -> root outline rels\n",
    "                s.run(UPSERT_OUTLINE_BOOK_REL_CYPHER, book_id=book_id, items=items_payload)\n",
    "                # 3) parent -> child rels\n",
    "                s.run(UPSERT_OUTLINE_PARENT_REL_CYPHER, items=items_payload)\n",
    "\n",
    "                # Link COVERS -> Page for every page in range\n",
    "                covers_rows = []\n",
    "                for row in items_payload:\n",
    "                    for pno in range(row[\"start_page\"], row[\"end_page\"] + 1):\n",
    "                        covers_rows.append({\"outline_id\": row[\"outline_id\"], \"page_no\": pno})\n",
    "\n",
    "                RANGE_BATCH = 2000\n",
    "                for k in range(0, len(covers_rows), RANGE_BATCH):\n",
    "                    part = covers_rows[k : k + RANGE_BATCH]\n",
    "                    s.run(LINK_COVERS_CYPHER, book_id=book_id, items=part)\n",
    "\n",
    "            # Chunks\n",
    "            for p in pages_payload:\n",
    "                page_no = p[\"page_no\"]\n",
    "                chunks = chunk_text(p[\"text\"])\n",
    "                for j, ct in enumerate(chunks):\n",
    "                    chunk_id = f\"{book_id}:p{page_no}:c{j}:{sha1(ct)[:10]}\"\n",
    "                    chunk_rows.append(\n",
    "                        {\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"page_no\": page_no,\n",
    "                            \"chunk_index\": j,\n",
    "                            \"text\": ct,\n",
    "                            \"checksum\": sha1(ct),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            CHUNK_BATCH = 1000\n",
    "            for k in range(0, len(chunk_rows), CHUNK_BATCH):\n",
    "                s.run(\n",
    "                    UPSERT_CHUNKS_CYPHER,\n",
    "                    book_id=book_id,\n",
    "                    chunks=chunk_rows[k : k + CHUNK_BATCH],\n",
    "                )\n",
    "\n",
    "            # If we have outline, create SUPPORTED_BY edges (Outline -> Chunk)\n",
    "            if toc_items:\n",
    "                s.run(SUPPORTED_BY_CYPHER)\n",
    "                        # v2 deeper layer: materialize Concept nodes + attach them\n",
    "            if toc_items:\n",
    "                s.run(CONSTRAINTS_V2_CYPHER)\n",
    "                s.run(UPSERT_CONCEPTS_FROM_OUTLINE_CYPHER, book_id=book_id)\n",
    "                s.run(LINK_OUTLINE_HAS_CONCEPT_CYPHER, book_id=book_id)\n",
    "                s.run(LINK_CHUNK_ABOUT_CONCEPT_CYPHER, book_id=book_id)\n",
    "\n",
    "        # ---- Embeddings (Gemini) and write to Neo4j\n",
    "        gemini_client = genai.Client()  # picks up GEMINI_API_KEY automatically  [oai_citation:2‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "\n",
    "        def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "            # Keep 1536 dims so you don't need to change your Neo4j vector index settings\n",
    "            # Gemini defaults to 3072, but output_dimensionality can be set.  [oai_citation:3‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "            result = gemini_client.models.embed_content(\n",
    "                model=\"text-embedding-004\",\n",
    "                contents=texts,  # list batching supported  [oai_citation:4‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "                config=types.EmbedContentConfig(\n",
    "                    task_type=\"RETRIEVAL_DOCUMENT\",   # correct for indexing chunks  [oai_citation:5‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "                    output_dimensionality=1536        # 768/1536/3072 recommended  [oai_citation:6‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "                ),\n",
    "            )\n",
    "            return [e.values for e in result.embeddings]\n",
    "\n",
    "        # embed only chunks missing embeddings\n",
    "        with driver.session() as s:\n",
    "            missing = s.run(\n",
    "                \"MATCH (c:Chunk) WHERE c.book_id=$book_id AND c.embedding IS NULL \"\n",
    "                \"RETURN c.chunk_id AS id, c.text AS text \"\n",
    "                \"ORDER BY c.page_no, c.chunk_index\",\n",
    "                book_id=book_id,\n",
    "            ).data()\n",
    "\n",
    "        rows = [{\"chunk_id\": r[\"id\"], \"text\": r[\"text\"]} for r in missing]\n",
    "\n",
    "        for i in range(0, len(rows), embed_batch):\n",
    "            group = rows[i : i + embed_batch]\n",
    "            vectors = embed_texts([g[\"text\"] for g in group])\n",
    "            payload = [{\"chunk_id\": g[\"chunk_id\"], \"embedding\": v} for g, v in zip(group, vectors)]\n",
    "\n",
    "            with driver.session() as s:\n",
    "                s.run(SET_EMBEDDINGS_CYPHER, rows=payload)\n",
    "\n",
    "        print(f\"Done. Ingested book_id={book_id}, pages={total_pages}, chunks={len(chunk_rows)}.\")\n",
    "        return {\n",
    "            \"book_id\": book_id,\n",
    "            \"pages\": total_pages,\n",
    "            \"chunks\": len(chunk_rows),\n",
    "            \"embedded\": len(rows),\n",
    "            \"has_toc_outline\": bool(toc_items),\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a2de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Ingested book_id=pindyck_micro_9e, pages=787, chunks=1387.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'book_id': 'pindyck_micro_9e',\n",
       " 'pages': 787,\n",
       " 'chunks': 1387,\n",
       " 'embedded': 1387,\n",
       " 'has_toc_outline': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.environ[\"GEMINI_API_KEY\"] = \n",
    "\n",
    "result = run_ingestion(\n",
    "    pdf=\"./data/(The Pearson series in economics) Pindyck, Robert S._ Rubinfeld, Daniel L - Microeconomics-Pearson (2018).pdf\",\n",
    "    book_id=\"pindyck_micro_9e\",\n",
    "    title=\"Microeconomics\",\n",
    "    authors=\"Pindyck, Rubinfeld\",\n",
    "    edition=\"9\",\n",
    "    year=\"2018\",\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_pass=\"testpassword\",\n",
    "    embed_model=\"text-embedding-3-small\",\n",
    "    embed_batch=64,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdf3c4",
   "metadata": {},
   "source": [
    "concept layer + exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7514b400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept constraints/indexes created (or already existed).\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI  = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"testpassword\"\n",
    "\n",
    "def run_one(uri, user, password, cypher, **params):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    try:\n",
    "        with driver.session() as s:\n",
    "            return s.run(cypher, **params).data()\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "schema_statements = [\n",
    "    # Unique concept per book\n",
    "    \"\"\"\n",
    "    CREATE CONSTRAINT concept_key IF NOT EXISTS\n",
    "    FOR (c:Concept) REQUIRE (c.book_id, c.name) IS UNIQUE\n",
    "    \"\"\",\n",
    "\n",
    "    # Helpful lookup index\n",
    "    \"\"\"\n",
    "    CREATE INDEX concept_name IF NOT EXISTS\n",
    "    FOR (c:Concept) ON (c.name)\n",
    "    \"\"\",\n",
    "\n",
    "    # Fulltext index for chunk text\n",
    "    \"\"\"\n",
    "    CREATE FULLTEXT INDEX chunkText IF NOT EXISTS\n",
    "    FOR (c:Chunk) ON EACH [c.text]\n",
    "    \"\"\",\n",
    "\n",
    "    # Fulltext for outline titles (optional)\n",
    "    \"\"\"\n",
    "    CREATE FULLTEXT INDEX outlineTitle IF NOT EXISTS\n",
    "    FOR (o:Outline) ON EACH [o.title]\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "for stmt in schema_statements:\n",
    "    run_one(NEO4J_URI, NEO4J_USER, NEO4J_PASS, stmt)\n",
    "\n",
    "print(\"Concept constraints/indexes created (or already existed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23f928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def run_cypher(uri: str, user: str, password: str, cypher: str, **params):\n",
    "    \"\"\"\n",
    "    Convenience helper: run a Cypher query and return rows as a list[dict].\n",
    "    \"\"\"\n",
    "    drv = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with drv.session() as sess:\n",
    "        res = sess.run(cypher, **params)\n",
    "        rows = [r.data() for r in res]\n",
    "    drv.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caa5fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded concepts: 455\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "BOOK_ID = \"pindyck_micro_9e\"\n",
    "\n",
    "# 1) Pull outline titles\n",
    "outline_rows = run_cypher(\n",
    "    NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "    \"\"\"\n",
    "    MATCH (o:Outline)\n",
    "    WHERE o.title IS NOT NULL\n",
    "    RETURN o.outline_id AS outline_id, o.title AS title, o.level AS level, o.kind AS kind\n",
    "    ORDER BY o.level, o.title\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def normalize_title_to_concepts(title: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Conservative extraction:\n",
    "      - remove leading numbering (\"1.\", \"1.2\", \"CHAPTER 3\", \"Appendix A\", roman numerals)\n",
    "      - split on ':' or '—' to keep main + subtitle as separate candidates\n",
    "      - keep phrases of reasonable length\n",
    "    \"\"\"\n",
    "    t = title.strip()\n",
    "\n",
    "    # drop common prefixes\n",
    "    t = re.sub(r\"^\\s*(chapter|ch\\.)\\s*\\d+\\s*[:\\-–—]?\\s*\", \"\", t, flags=re.I)\n",
    "    t = re.sub(r\"^\\s*(appendix)\\s*[A-Z0-9]+\\s*[:\\-–—]?\\s*\", \"\", t, flags=re.I)\n",
    "    t = re.sub(r\"^\\s*\\d+(\\.\\d+)*\\s*[:\\-–—]?\\s*\", \"\", t)     # 1.2.3 Title\n",
    "    t = re.sub(r\"^\\s*[IVXLCDM]+\\.\\s*\", \"\", t, flags=re.I)   # Roman numeral headings\n",
    "\n",
    "    # split on colon/dash variants into up to 2 parts\n",
    "    parts = re.split(r\"\\s*[:\\-–—]\\s*\", t, maxsplit=1)\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "    concepts = []\n",
    "    for p in parts:\n",
    "        # filter overly short / long\n",
    "        if 3 <= len(p) <= 80:\n",
    "            # normalize whitespace\n",
    "            p2 = re.sub(r\"\\s+\", \" \", p).strip()\n",
    "            # avoid pure punctuation/numbers\n",
    "            if re.search(r\"[A-Za-z]\", p2):\n",
    "                concepts.append({\"name\": p2, \"aliases\": []})\n",
    "    # de-dup by lowercase\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in concepts:\n",
    "        key = c[\"name\"].lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(c)\n",
    "    return out\n",
    "\n",
    "rows_to_upsert = []\n",
    "for r in outline_rows:\n",
    "    for c in normalize_title_to_concepts(r[\"title\"]):\n",
    "        rows_to_upsert.append({\n",
    "            \"book_id\": BOOK_ID,\n",
    "            \"outline_id\": r[\"outline_id\"],\n",
    "            \"name\": c[\"name\"],\n",
    "            \"aliases\": c[\"aliases\"],\n",
    "        })\n",
    "\n",
    "# 2) Upsert Concept + link Outline-ABOUT-Concept\n",
    "UPSERT_CONCEPTS_FROM_OUTLINE = \"\"\"\n",
    "UNWIND $rows AS row\n",
    "MERGE (c:Concept {book_id: row.book_id, name: row.name})\n",
    "ON CREATE SET c.aliases = row.aliases\n",
    "WITH c, row\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MERGE (o)-[:ABOUT]->(c);\n",
    "\"\"\"\n",
    "\n",
    "run_cypher(NEO4J_URI, NEO4J_USER, NEO4J_PASS, UPSERT_CONCEPTS_FROM_OUTLINE, rows=rows_to_upsert)\n",
    "\n",
    "# sanity counts\n",
    "counts = run_cypher(\n",
    "    NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "    \"\"\"\n",
    "    MATCH (c:Concept {book_id:$book_id}) RETURN count(c) AS concepts;\n",
    "    \"\"\",\n",
    "    book_id=BOOK_ID\n",
    ")\n",
    "print(\"Seeded concepts:\", counts[0][\"concepts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd43d955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MENTIONS edges created: 2225\n"
     ]
    }
   ],
   "source": [
    "TOPK_CHUNKS_PER_CONCEPT = 40   # start small; raise later\n",
    "MAX_CONCEPTS_PER_RUN    = 200  # incremental; raise later\n",
    "\n",
    "# Get a batch of concepts for this book\n",
    "concepts = run_cypher(\n",
    "    NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "    \"\"\"\n",
    "    MATCH (c:Concept {book_id:$book_id})\n",
    "    RETURN c.name AS name\n",
    "    ORDER BY c.name\n",
    "    LIMIT $limit\n",
    "    \"\"\",\n",
    "    book_id=BOOK_ID, limit=MAX_CONCEPTS_PER_RUN\n",
    ")\n",
    "\n",
    "LINK_MENTIONS_FULLTEXT = \"\"\"\n",
    "MATCH (c:Concept {book_id:$book_id, name:$concept_name})\n",
    "CALL db.index.fulltext.queryNodes('chunkText', $q) YIELD node, score\n",
    "WHERE node.book_id = $book_id\n",
    "WITH c, node, score\n",
    "ORDER BY score DESC\n",
    "LIMIT $k\n",
    "MERGE (node)-[r:MENTIONS]->(c)\n",
    "ON CREATE SET r.method = 'fulltext', r.score = score;\n",
    "\"\"\"\n",
    "\n",
    "for c in concepts:\n",
    "    name = c[\"name\"]\n",
    "    # Quote the concept for phrase search; this is Lucene syntax.\n",
    "    q = f\"\\\"{name}\\\"\"\n",
    "    run_cypher(\n",
    "        NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "        LINK_MENTIONS_FULLTEXT,\n",
    "        book_id=BOOK_ID,\n",
    "        concept_name=name,\n",
    "        q=q,                      # <-- renamed from query=\n",
    "        k=TOPK_CHUNKS_PER_CONCEPT\n",
    "    )\n",
    "\n",
    "# sanity\n",
    "m = run_cypher(\n",
    "    NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "    \"\"\"\n",
    "    MATCH (:Chunk {book_id:$book_id})-[r:MENTIONS]->(:Concept {book_id:$book_id})\n",
    "    RETURN count(r) AS mention_edges;\n",
    "    \"\"\",\n",
    "    book_id=BOOK_ID\n",
    ")\n",
    "\n",
    "print(\"MENTIONS edges created:\", m[0][\"mention_edges\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58d31aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts after expansion: 646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Pull some \"high-signal\" chunks: chunks that mention at least one concept\n",
    "chunk_sample = run_cypher(\n",
    "    NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "    \"\"\"\n",
    "    MATCH (ch:Chunk {book_id:$book_id})-[:MENTIONS]->(:Concept {book_id:$book_id})\n",
    "    RETURN ch.text AS text\n",
    "    LIMIT 300\n",
    "    \"\"\",\n",
    "    book_id=BOOK_ID\n",
    ")\n",
    "texts = [r[\"text\"] for r in chunk_sample if r.get(\"text\")]\n",
    "\n",
    "if texts:\n",
    "    vec = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(2, 3),\n",
    "        max_features=6000,\n",
    "        min_df=2\n",
    "    )\n",
    "    X = vec.fit_transform(texts)\n",
    "    terms = vec.get_feature_names_out()\n",
    "    scores = X.mean(axis=0).A1\n",
    "    ranked = sorted(zip(terms, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # conservative filter\n",
    "    new_terms = []\n",
    "    for t, s in ranked[:200]:\n",
    "        if 6 <= len(t) <= 50 and re.search(r\"[a-zA-Z]\", t):\n",
    "            # avoid junk patterns\n",
    "            if not re.search(r\"\\b(fig|table|chapter|section)\\b\", t.lower()):\n",
    "                new_terms.append(t.strip())\n",
    "\n",
    "    # upsert as new Concepts (no outline link)\n",
    "    upsert_rows = [{\"book_id\": BOOK_ID, \"name\": t, \"aliases\": []} for t in new_terms]\n",
    "\n",
    "    UPSERT_CONCEPTS_ONLY = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (c:Concept {book_id: row.book_id, name: row.name})\n",
    "    ON CREATE SET c.aliases = row.aliases;\n",
    "    \"\"\"\n",
    "\n",
    "    run_cypher(NEO4J_URI, NEO4J_USER, NEO4J_PASS, UPSERT_CONCEPTS_ONLY, rows=upsert_rows)\n",
    "\n",
    "    ccount = run_cypher(\n",
    "        NEO4J_URI, NEO4J_USER, NEO4J_PASS,\n",
    "        \"MATCH (c:Concept {book_id:$book_id}) RETURN count(c) AS concepts;\",\n",
    "        book_id=BOOK_ID\n",
    "    )\n",
    "    print(\"Concepts after expansion:\", ccount[0][\"concepts\"])\n",
    "else:\n",
    "    print(\"No chunk sample available (need MENTIONS edges first).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
