{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fab7ad",
   "metadata": {},
   "source": [
    "Conceptually: strong idea, and the “mixture of experts” framing maps very naturally to how economists actually work (parse → frame → model → solve → interpret → critique assumptions). The main question is implementation strategy so you get something usable quickly without getting trapped in “train-a-giant-model-from-scratch” territory.\n",
    "\n",
    "1) What I think works best in practice (and why)\n",
    "\n",
    "A. Start as agentic MoE (router + specialist experts), not “true MoE pretraining”\n",
    "\n",
    "“True” MoE (multiple expert subnetworks inside one transformer trained jointly) is powerful but expensive and operationally heavy. You can get 80–90% of the benefit by doing:\n",
    "\t•\tA Router/Framer model that classifies + structures the task\n",
    "\t•\tSpecialist expert models (or LoRA adapters) per subfield/task\n",
    "\t•\tA tool/code expert that writes and executes code\n",
    "\t•\tA critic that checks assumptions, units, equilibrium conditions, identification strategy, etc.\n",
    "\n",
    "This is also far easier to iterate on, evaluate, and swap components.\n",
    "\n",
    "B. Use RAG + structured knowledge as the foundation, fine-tuning second\n",
    "\n",
    "For economics, correctness often depends on definitions, assumptions, and canonical results. RAG over your curated sources plus a knowledge graph of concepts is extremely effective, and it avoids “baking in” errors.\n",
    "\n",
    "Then fine-tune (LoRA/QLoRA) to improve:\n",
    "\t•\tframing templates\n",
    "\t•\tstep ordering\n",
    "\t•\tstyle (econ-precise)\n",
    "\t•\tconsistent notation\n",
    "\t•\tcode patterns\n",
    "\n",
    "C. You should treat “econ ML expert → code” as a first-class toolchain\n",
    "\n",
    "Make code generation verifiable:\n",
    "\t•\tgenerated code runs in a sandbox (Python)\n",
    "\t•\tincludes tests or sanity checks (units, signs, boundary cases)\n",
    "\t•\tproduces plots/tables where relevant\n",
    "\t•\tlogs decisions and assumptions\n",
    "\n",
    "That makes the system materially better than a “smart text bot”.\n",
    "\n",
    "2) Data plan: what to use and what to avoid\n",
    "\n",
    "What’s ideal\n",
    "\t•\tYour own notes, homework solutions, summaries, derivations, cheat sheets\n",
    "\t•\tProblem sets where you have the right to use the material\n",
    "\t•\tPublic-domain / permissively licensed resources\n",
    "\t•\tSelf-generated Q/A pairs from your notes (with careful review)\n",
    "\n",
    "What’s risky\n",
    "\t•\tTextbooks: typically copyrighted. Using them for training is a legal/licensing question.\n",
    "\t•\tQuizlet: content and access are governed by their Terms; scraping or bulk use can be a problem, and sets may be copyrighted by the creators.\n",
    "\n",
    "Pragmatic approach:\n",
    "\t•\tUse copyrighted sources for private RAG if you have lawful access and keep it internal (still not legal advice), but avoid training directly on them unless you have permission.\n",
    "\t•\tPrefer converting what you learn into your own “canonical cards” (definitions, theorems, standard derivations, solved examples) and train on those.\n",
    "\n",
    "3) Proposed expert lineup (a concrete blueprint)\n",
    "\t1.\tProblem Digestor (Econ Parser)\n",
    "\n",
    "\t•\tExtracts: agents, goods, constraints, timing, information, equilibrium concept\n",
    "\t•\tDetects: “this is actually a mechanism design / adverse selection / IV / general equilibrium / game” etc.\n",
    "\t•\tOutput format: a structured schema (JSON)\n",
    "\n",
    "\t2.\tField Router\n",
    "\n",
    "\t•\tMulti-label classification: micro theory, macro, IO, metrics, finance, public, labor, trade, dev, behavioral, etc.\n",
    "\t•\tRoutes to 1–3 specialists, not just one\n",
    "\n",
    "\t3.\tModel Framer\n",
    "\n",
    "\t•\tSelect canonical model class + assumptions\n",
    "\t•\tOffers alternatives (e.g., partial vs general equilibrium; static vs dynamic; OLS vs IV vs DiD)\n",
    "\n",
    "\t4.\tSolver / Derivation Expert\n",
    "\n",
    "\t•\tProduces the math (FOCs, comparative statics, equilibrium existence, welfare)\n",
    "\n",
    "\t5.\tEmpirical Strategy Expert (Metrics)\n",
    "\n",
    "\t•\tIdentification strategy, threats, robustness, DAG thinking\n",
    "\t•\tSuggests data transforms + estimator choice\n",
    "\n",
    "\t6.\tCode Expert\n",
    "\n",
    "\t•\tTurns the framed model/estimation into reproducible code\n",
    "\t•\tAdds checks, diagnostics, and figures\n",
    "\n",
    "\t7.\tCritic / Referee #2\n",
    "\n",
    "\t•\tAssumption audit, sign checks, edge cases\n",
    "\t•\t“What would a journal referee attack?”\n",
    "\n",
    "This is exactly the kind of structure that benefits from a router + specialists.\n",
    "\n",
    "4) Minimum Viable Product I’d build first (fast path)\n",
    "\n",
    "MVP goal: A system that takes any econ question and returns:\n",
    "\t•\tstructured framing (schema)\n",
    "\t•\trecommended approach (model/estimator)\n",
    "\t•\tsolution sketch (math or logic)\n",
    "\t•\toptional runnable code (if asked)\n",
    "\t•\ta short “assumptions & failure modes” section\n",
    "\n",
    "Implementation: start with one strong base LLM + prompt-routed agents + RAG over your notes.\n",
    "\n",
    "Then iterate:\n",
    "\t•\tfine-tune router (cheap)\n",
    "\t•\tfine-tune “framer” and “critic” on your canonical format\n",
    "\t•\tlater: LoRA adapters per subfield\n",
    "\n",
    "5) The hardest parts (so you don’t get blindsided)\n",
    "\t•\tEvaluation: you need a gold set of questions with expected framing and key points. Without this, progress becomes vibes-based.\n",
    "\t•\tNotation consistency: econ is fragile to notation drift; solve with a global style guide + schema + critic enforcement.\n",
    "\t•\tData cleanliness: your notes/homework will be noisy. The trick is converting them into clean, structured “cards” and “worked examples”.\n",
    "\t•\tCopyright/ToS: biggest non-technical risk. Design the pipeline so you can swap in compliant datasets easily.\n",
    "\n",
    "6) If you want my recommendation in one line\n",
    "\n",
    "Build it as an agentic MoE with a strong router + RAG + verifier, then add LoRA specialists once you’ve locked the schema and evaluation suite.\n",
    "\n",
    "If you reply with (a) which subfields you care about most (top 5) and (b) your intended runtime stack (local Ollama vs API vs server GPUs), I’ll propose a concrete architecture diagram + repo structure + the exact schemas/prompts for each expert so you can start implementing immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a05f0c",
   "metadata": {},
   "source": [
    "pip install pymupdf neo4j openai tiktoken\n",
    "export OPENAI_API_KEY=\"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\"\n",
    "gemini AIzaSyAODasJVvnnhDYAu7-qh9B7jZKbfo-3B7w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52b021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from neo4j import GraphDatabase\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DEFAULT_EMBED_MODEL = \"text-embedding-3-small\"  # 1536 dims in common usage\n",
    "# OpenAI embeddings guide lists text-embedding-3-small / 3-large as current.   [oai_citation:4‡OpenAI Platform](https://platform.openai.com/docs/guides/embeddings?utm_source=chatgpt.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "960b4a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = re.sub(r\"[ \\t]+\\n\", \"\\n\", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def chunk_text(text: str, target_chars: int = 2500, overlap_chars: int = 250) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simple, robust chunker (char-based) that tries to keep paragraphs together.\n",
    "    Good enough for v0; you can swap to token-based later.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n\", text) if p.strip()]\n",
    "    chunks = []\n",
    "    cur = \"\"\n",
    "\n",
    "    for p in paras:\n",
    "        if len(cur) + len(p) + 2 <= target_chars:\n",
    "            cur = (cur + \"\\n\\n\" + p).strip() if cur else p\n",
    "        else:\n",
    "            if cur:\n",
    "                chunks.append(cur)\n",
    "            # if paragraph is huge, hard-split\n",
    "            if len(p) > target_chars:\n",
    "                start = 0\n",
    "                while start < len(p):\n",
    "                    end = min(len(p), start + target_chars)\n",
    "                    chunks.append(p[start:end])\n",
    "                    start = max(end - overlap_chars, end)\n",
    "                cur = \"\"\n",
    "            else:\n",
    "                cur = p\n",
    "\n",
    "    if cur:\n",
    "        chunks.append(cur)\n",
    "\n",
    "    # add overlap between chunks\n",
    "    if overlap_chars > 0 and len(chunks) > 1:\n",
    "        out = []\n",
    "        for i, c in enumerate(chunks):\n",
    "            if i == 0:\n",
    "                out.append(c)\n",
    "            else:\n",
    "                prev = out[-1]\n",
    "                overlap = prev[-overlap_chars:] if len(prev) > overlap_chars else prev\n",
    "                out.append((overlap + \"\\n\\n\" + c).strip())\n",
    "        return out\n",
    "\n",
    "    return chunks\n",
    "\n",
    "@dataclass\n",
    "class TocItem:\n",
    "    level: int\n",
    "    title: str\n",
    "    start_page: int  # 1-based page number (matches PyMuPDF TOC)\n",
    "    end_page: int    # computed\n",
    "\n",
    "def compute_end_pages(toc: List[Tuple[int, str, int]], total_pages: int) -> List[TocItem]:\n",
    "    \"\"\"\n",
    "    toc is list of (level, title, page) with 1-based page numbers\n",
    "    end_page is next item page - 1 at same or higher level, else total_pages\n",
    "    \"\"\"\n",
    "    items = [TocItem(lvl, title.strip(), int(pg), total_pages) for (lvl, title, pg) in toc]\n",
    "    for i in range(len(items)):\n",
    "        this = items[i]\n",
    "        end = total_pages\n",
    "        for j in range(i + 1, len(items)):\n",
    "            nxt = items[j]\n",
    "            if nxt.level <= this.level:\n",
    "                end = max(this.start_page, nxt.start_page - 1)\n",
    "                break\n",
    "        items[i].end_page = end\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b619c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Neo4j writers (NO APOC)\n",
    "# -----------------------------\n",
    "\n",
    "SCHEMA_CYPHER = \"\"\"\n",
    "MERGE (b:Book {book_id: $book_id})\n",
    "SET b.title = $title,\n",
    "    b.authors = $authors,\n",
    "    b.edition = $edition,\n",
    "    b.year = $year,\n",
    "    b.total_pages = $total_pages;\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_PAGES_CYPHER = \"\"\"\n",
    "UNWIND $pages AS row\n",
    "MERGE (p:Page {book_id: $book_id, page_no: row.page_no})\n",
    "SET p.text = row.text,\n",
    "    p.checksum = row.checksum\n",
    "WITH p\n",
    "MATCH (b:Book {book_id: $book_id})\n",
    "MERGE (b)-[:HAS_PAGE]->(p);\n",
    "\"\"\"\n",
    "\n",
    "# Outline ingestion in 3 passes (Neo4j 5 safe; no MATCH inside FOREACH)\n",
    "UPSERT_OUTLINE_NODES_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "MERGE (o:Outline {outline_id: row.outline_id})\n",
    "SET o.kind = row.kind,\n",
    "    o.level = row.level,\n",
    "    o.title = row.title,\n",
    "    o.start_page = row.start_page,\n",
    "    o.end_page = row.end_page;\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_OUTLINE_BOOK_REL_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "WITH row\n",
    "WHERE row.parent_outline_id IS NULL\n",
    "MATCH (b:Book {book_id: $book_id})\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MERGE (b)-[:HAS_OUTLINE]->(o);\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_OUTLINE_PARENT_REL_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "WITH row\n",
    "WHERE row.parent_outline_id IS NOT NULL\n",
    "MATCH (parent:Outline {outline_id: row.parent_outline_id})\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MERGE (parent)-[:HAS_CHILD]->(o);\n",
    "\"\"\"\n",
    "\n",
    "LINK_COVERS_CYPHER = \"\"\"\n",
    "UNWIND $items AS row\n",
    "MATCH (o:Outline {outline_id: row.outline_id})\n",
    "MATCH (p:Page {book_id: $book_id, page_no: row.page_no})\n",
    "MERGE (o)-[:COVERS]->(p);\n",
    "\"\"\"\n",
    "\n",
    "UPSERT_CHUNKS_CYPHER = \"\"\"\n",
    "UNWIND $chunks AS row\n",
    "MERGE (c:Chunk {chunk_id: row.chunk_id})\n",
    "SET c.book_id = $book_id,\n",
    "    c.page_no = row.page_no,\n",
    "    c.chunk_index = row.chunk_index,\n",
    "    c.text = row.text,\n",
    "    c.checksum = row.checksum\n",
    "WITH c, row\n",
    "MATCH (p:Page {book_id: $book_id, page_no: row.page_no})\n",
    "MERGE (p)-[:HAS_CHUNK]->(c);\n",
    "\"\"\"\n",
    "\n",
    "# Store embeddings as LIST<FLOAT> property (no procedures)\n",
    "SET_EMBEDDINGS_CYPHER = \"\"\"\n",
    "UNWIND $rows AS row\n",
    "MATCH (c:Chunk {chunk_id: row.chunk_id})\n",
    "SET c.embedding = row.embedding\n",
    "RETURN count(*) AS updated;\n",
    "\"\"\"\n",
    "\n",
    "SUPPORTED_BY_CYPHER = \"\"\"\n",
    "MATCH (o:Outline)-[:COVERS]->(:Page)-[:HAS_CHUNK]->(c:Chunk)\n",
    "MERGE (o)-[:SUPPORTED_BY]->(c);\n",
    "\"\"\"\n",
    "\n",
    "def kind_for_level(level: int) -> str:\n",
    "    if level == 1: return \"chapter\"\n",
    "    if level == 2: return \"subchapter\"\n",
    "    if level == 3: return \"topic\"\n",
    "    return \"subtopic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccea383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--pdf\", required=True, help=\"Path to textbook PDF\")\n",
    "    ap.add_argument(\"--book-id\", required=True)\n",
    "    ap.add_argument(\"--title\", required=True)\n",
    "    ap.add_argument(\"--authors\", default=\"\")\n",
    "    ap.add_argument(\"--edition\", default=\"\")\n",
    "    ap.add_argument(\"--year\", default=\"\")\n",
    "    ap.add_argument(\"--neo4j-uri\", default=\"bolt://localhost:7687\")\n",
    "    ap.add_argument(\"--neo4j-user\", default=\"neo4j\")\n",
    "    ap.add_argument(\"--neo4j-pass\", default=\"testpassword\")\n",
    "    ap.add_argument(\"--embed-model\", default=DEFAULT_EMBED_MODEL)\n",
    "    ap.add_argument(\"--embed-batch\", type=int, default=64)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.pdf):\n",
    "        raise FileNotFoundError(args.pdf)\n",
    "\n",
    "    # ---- Read PDF\n",
    "    doc = fitz.open(args.pdf)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    # ---- Extract TOC (best-case: PDF has a real outline)\n",
    "    raw_toc = doc.get_toc(simple=True)  # [(level, title, pageNo), ...]\n",
    "    toc_items: List[TocItem] = []\n",
    "    if raw_toc:\n",
    "        toc_items = compute_end_pages(raw_toc, total_pages)\n",
    "\n",
    "    # ---- Extract pages\n",
    "    pages_payload = []\n",
    "    for i in range(total_pages):\n",
    "        page_no = i + 1\n",
    "        text = clean_text(doc.load_page(i).get_text(\"text\"))\n",
    "        pages_payload.append({\n",
    "            \"page_no\": page_no,\n",
    "            \"text\": text,\n",
    "            \"checksum\": sha1(text),\n",
    "        })\n",
    "\n",
    "    # ---- Connect Neo4j\n",
    "    driver = GraphDatabase.driver(args.neo4j_uri, auth=(args.neo4j_user, args.neo4j_pass))\n",
    "\n",
    "    with driver.session() as s:\n",
    "        s.run(SET_EMBEDDINGS_CYPHER, rows=payload)\n",
    "\n",
    "        # Pages (batch)\n",
    "        s.run(UPSERT_PAGES_CYPHER, book_id=args.book_id, pages=pages_payload)\n",
    "\n",
    "        # Outline + parent-child\n",
    "        if toc_items:\n",
    "            items_payload = []\n",
    "            parent_stack: Dict[int, str] = {}  # level -> outline_id\n",
    "            for idx, it in enumerate(toc_items):\n",
    "                outline_id = f\"{args.book_id}:toc:{idx}:{sha1(it.title)[:10]}\"\n",
    "                parent_id = parent_stack.get(it.level - 1)\n",
    "                parent_stack[it.level] = outline_id\n",
    "                # truncate deeper levels when we go up\n",
    "                for lvl in list(parent_stack.keys()):\n",
    "                    if lvl > it.level:\n",
    "                        del parent_stack[lvl]\n",
    "\n",
    "                items_payload.append({\n",
    "                    \"outline_id\": outline_id,\n",
    "                    \"parent_outline_id\": parent_id,\n",
    "                    \"kind\": kind_for_level(it.level),\n",
    "                    \"level\": it.level,\n",
    "                    \"title\": it.title,\n",
    "                    \"start_page\": it.start_page,\n",
    "                    \"end_page\": it.end_page,\n",
    "                })\n",
    "\n",
    "            s.run(UPSERT_OUTLINE_CYPHER, book_id=args.book_id, items=items_payload)\n",
    "\n",
    "            # Link COVERS -> Page for every page in range\n",
    "            # For v0 we create explicit links to pages (fine for one book).\n",
    "            covers_rows = []\n",
    "            for row in items_payload:\n",
    "                for p in range(row[\"start_page\"], row[\"end_page\"] + 1):\n",
    "                    covers_rows.append({\"outline_id\": row[\"outline_id\"], \"page_no\": p})\n",
    "            # chunk this to avoid huge tx\n",
    "            CHUNK = 2000\n",
    "            for k in range(0, len(covers_rows), CHUNK):\n",
    "                part = covers_rows[k:k+CHUNK]\n",
    "                s.run(LINK_COVERS_CYPHER, book_id=args.book_id, items=part)\n",
    "\n",
    "        # Chunks\n",
    "        chunk_rows = []\n",
    "        for p in pages_payload:\n",
    "            page_no = p[\"page_no\"]\n",
    "            chunks = chunk_text(p[\"text\"])\n",
    "            for j, ct in enumerate(chunks):\n",
    "                chunk_id = f\"{args.book_id}:p{page_no}:c{j}:{sha1(ct)[:10]}\"\n",
    "                chunk_rows.append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"page_no\": page_no,\n",
    "                    \"chunk_index\": j,\n",
    "                    \"text\": ct,\n",
    "                    \"checksum\": sha1(ct),\n",
    "                })\n",
    "\n",
    "        # Insert chunks in manageable batches\n",
    "        CHUNK = 1000\n",
    "        for k in range(0, len(chunk_rows), CHUNK):\n",
    "            s.run(UPSERT_CHUNKS_CYPHER, book_id=args.book_id, chunks=chunk_rows[k:k+CHUNK])\n",
    "\n",
    "        # If we have outline, create SUPPORTED_BY edges (Outline -> Chunk)\n",
    "        if toc_items:\n",
    "            s.run(SUPPORTED_BY_CYPHER)\n",
    "\n",
    "    # ---- Embeddings (OpenAI) and write to Neo4j\n",
    "    client = OpenAI()\n",
    "    batch = args.embed_batch\n",
    "\n",
    "    def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "        resp = client.embeddings.create(model=args.embed_model, input=texts)\n",
    "        return [d.embedding for d in resp.data]\n",
    "\n",
    "    # embed only chunks missing embeddings\n",
    "    with driver.session() as s:\n",
    "        missing = s.run(\n",
    "            \"MATCH (c:Chunk) WHERE c.book_id=$book_id AND c.embedding IS NULL RETURN c.chunk_id AS id, c.text AS text\",\n",
    "            book_id=args.book_id,\n",
    "        ).data()\n",
    "\n",
    "    rows = [{\"chunk_id\": r[\"id\"], \"text\": r[\"text\"]} for r in missing]\n",
    "\n",
    "    for i in range(0, len(rows), batch):\n",
    "        group = rows[i:i+batch]\n",
    "        vectors = embed_texts([g[\"text\"] for g in group])\n",
    "        payload = [{\"chunk_id\": g[\"chunk_id\"], \"embedding\": v} for g, v in zip(group, vectors)]\n",
    "\n",
    "        with driver.session() as s:\n",
    "            try:\n",
    "                s.run(SET_EMBEDDINGS_CYPHER, rows=payload)\n",
    "            except Exception:\n",
    "                # fallback for older Neo4j builds where the procedure may be absent\n",
    "                s.run(SET_EMBEDDINGS_FALLBACK_CYPHER, rows=payload)\n",
    "\n",
    "    driver.close()\n",
    "    doc.close()\n",
    "\n",
    "    print(f\"Done. Ingested book_id={args.book_id}, pages={total_pages}, chunks={len(chunk_rows)}.\")\n",
    "    print(\"Reminder: CREATE VECTOR INDEX uses the embedding dimension you selected.\")\n",
    "    print(\"Vector index query procedure: CALL db.index.vector.queryNodes(...).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Notebook-friendly entrypoint (FIXED, Neo4j 5 safe, no APOC, no procedures)\n",
    "# -----------------------------\n",
    "def run_ingestion(\n",
    "    pdf: str,\n",
    "    book_id: str,\n",
    "    title: str,\n",
    "    authors: str = \"\",\n",
    "    edition: str = \"\",\n",
    "    year: str = \"\",\n",
    "    neo4j_uri: str = \"bolt://localhost:7687\",\n",
    "    neo4j_user: str = \"neo4j\",\n",
    "    neo4j_pass: str = \"testpassword\",\n",
    "    embed_model: str = DEFAULT_EMBED_MODEL,\n",
    "    embed_batch: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    Notebook-friendly runner. Mirrors the argparse main() behavior,\n",
    "    but takes explicit parameters.\n",
    "\n",
    "    Assumes these globals already exist in the notebook:\n",
    "      - SCHEMA_CYPHER, UPSERT_PAGES_CYPHER, LINK_COVERS_CYPHER, UPSERT_CHUNKS_CYPHER\n",
    "      - UPSERT_OUTLINE_NODES_CYPHER, UPSERT_OUTLINE_BOOK_REL_CYPHER, UPSERT_OUTLINE_PARENT_REL_CYPHER\n",
    "      - SUPPORTED_BY_CYPHER, SET_EMBEDDINGS_CYPHER\n",
    "      - sha1, clean_text, chunk_text, compute_end_pages, kind_for_level\n",
    "      - TocItem (dataclass), plus imports (os, fitz, GraphDatabase, OpenAI, Dict, List)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from typing import Dict, List\n",
    "\n",
    "    if not os.path.exists(pdf):\n",
    "        raise FileNotFoundError(pdf)\n",
    "\n",
    "    # ---- Read PDF\n",
    "    doc = fitz.open(pdf)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    # ---- Extract TOC (best-case: PDF has a real outline)\n",
    "    raw_toc = doc.get_toc(simple=True)  # [(level, title, pageNo), ...]\n",
    "    toc_items: List[TocItem] = []\n",
    "    if raw_toc:\n",
    "        toc_items = compute_end_pages(raw_toc, total_pages)\n",
    "\n",
    "    # ---- Extract pages\n",
    "    pages_payload = []\n",
    "    for i in range(total_pages):\n",
    "        page_no = i + 1\n",
    "        text = clean_text(doc.load_page(i).get_text(\"text\"))\n",
    "        pages_payload.append(\n",
    "            {\n",
    "                \"page_no\": page_no,\n",
    "                \"text\": text,\n",
    "                \"checksum\": sha1(text),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # ---- Connect Neo4j\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_pass))\n",
    "\n",
    "    # ---- Write Book, Pages, Outline, Chunks\n",
    "    chunk_rows = []  # keep for return value\n",
    "    try:\n",
    "        with driver.session() as s:\n",
    "            # Book node\n",
    "            s.run(\n",
    "                SCHEMA_CYPHER,\n",
    "                book_id=book_id,\n",
    "                title=title,\n",
    "                authors=authors,\n",
    "                edition=edition,\n",
    "                year=year,\n",
    "                total_pages=total_pages,\n",
    "            )\n",
    "\n",
    "            # Pages\n",
    "            s.run(UPSERT_PAGES_CYPHER, book_id=book_id, pages=pages_payload)\n",
    "\n",
    "            # Outline + parent-child (Neo4j 5 safe; no FOREACH-with-MATCH)\n",
    "            items_payload = []\n",
    "            if toc_items:\n",
    "                parent_stack: Dict[int, str] = {}  # level -> outline_id\n",
    "\n",
    "                for idx, it in enumerate(toc_items):\n",
    "                    outline_id = f\"{book_id}:toc:{idx}:{sha1(it.title)[:10]}\"\n",
    "                    parent_id = parent_stack.get(it.level - 1)\n",
    "\n",
    "                    parent_stack[it.level] = outline_id\n",
    "                    # remove deeper levels when moving up\n",
    "                    for lvl in list(parent_stack.keys()):\n",
    "                        if lvl > it.level:\n",
    "                            del parent_stack[lvl]\n",
    "\n",
    "                    items_payload.append(\n",
    "                        {\n",
    "                            \"outline_id\": outline_id,\n",
    "                            \"parent_outline_id\": parent_id,\n",
    "                            \"kind\": kind_for_level(it.level),\n",
    "                            \"level\": it.level,\n",
    "                            \"title\": it.title,\n",
    "                            \"start_page\": it.start_page,\n",
    "                            \"end_page\": it.end_page,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                # 1) nodes\n",
    "                s.run(UPSERT_OUTLINE_NODES_CYPHER, items=items_payload)\n",
    "                # 2) book -> root outline rels\n",
    "                s.run(UPSERT_OUTLINE_BOOK_REL_CYPHER, book_id=book_id, items=items_payload)\n",
    "                # 3) parent -> child rels\n",
    "                s.run(UPSERT_OUTLINE_PARENT_REL_CYPHER, items=items_payload)\n",
    "\n",
    "                # Link COVERS -> Page for every page in range\n",
    "                covers_rows = []\n",
    "                for row in items_payload:\n",
    "                    for pno in range(row[\"start_page\"], row[\"end_page\"] + 1):\n",
    "                        covers_rows.append({\"outline_id\": row[\"outline_id\"], \"page_no\": pno})\n",
    "\n",
    "                RANGE_BATCH = 2000\n",
    "                for k in range(0, len(covers_rows), RANGE_BATCH):\n",
    "                    part = covers_rows[k : k + RANGE_BATCH]\n",
    "                    s.run(LINK_COVERS_CYPHER, book_id=book_id, items=part)\n",
    "\n",
    "            # Chunks\n",
    "            for p in pages_payload:\n",
    "                page_no = p[\"page_no\"]\n",
    "                chunks = chunk_text(p[\"text\"])\n",
    "                for j, ct in enumerate(chunks):\n",
    "                    chunk_id = f\"{book_id}:p{page_no}:c{j}:{sha1(ct)[:10]}\"\n",
    "                    chunk_rows.append(\n",
    "                        {\n",
    "                            \"chunk_id\": chunk_id,\n",
    "                            \"page_no\": page_no,\n",
    "                            \"chunk_index\": j,\n",
    "                            \"text\": ct,\n",
    "                            \"checksum\": sha1(ct),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            CHUNK_BATCH = 1000\n",
    "            for k in range(0, len(chunk_rows), CHUNK_BATCH):\n",
    "                s.run(\n",
    "                    UPSERT_CHUNKS_CYPHER,\n",
    "                    book_id=book_id,\n",
    "                    chunks=chunk_rows[k : k + CHUNK_BATCH],\n",
    "                )\n",
    "\n",
    "            # If we have outline, create SUPPORTED_BY edges (Outline -> Chunk)\n",
    "            if toc_items:\n",
    "                s.run(SUPPORTED_BY_CYPHER)\n",
    "\n",
    "        # ---- Embeddings (Gemini) and write to Neo4j\n",
    "        gemini_client = genai.Client()  # picks up GEMINI_API_KEY automatically  [oai_citation:2‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "\n",
    "        def embed_texts(texts: List[str]) -> List[List[float]]:\n",
    "            # Keep 1536 dims so you don't need to change your Neo4j vector index settings\n",
    "            # Gemini defaults to 3072, but output_dimensionality can be set.  [oai_citation:3‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "            result = gemini_client.models.embed_content(\n",
    "                model=\"text-embedding-004\",\n",
    "                contents=texts,  # list batching supported  [oai_citation:4‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "                config=types.EmbedContentConfig(\n",
    "                    task_type=\"RETRIEVAL_DOCUMENT\",   # correct for indexing chunks  [oai_citation:5‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "                    output_dimensionality=1536        # 768/1536/3072 recommended  [oai_citation:6‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\n",
    "                ),\n",
    "            )\n",
    "            return [e.values for e in result.embeddings]\n",
    "\n",
    "        # embed only chunks missing embeddings\n",
    "        with driver.session() as s:\n",
    "            missing = s.run(\n",
    "                \"MATCH (c:Chunk) WHERE c.book_id=$book_id AND c.embedding IS NULL \"\n",
    "                \"RETURN c.chunk_id AS id, c.text AS text \"\n",
    "                \"ORDER BY c.page_no, c.chunk_index\",\n",
    "                book_id=book_id,\n",
    "            ).data()\n",
    "\n",
    "        rows = [{\"chunk_id\": r[\"id\"], \"text\": r[\"text\"]} for r in missing]\n",
    "\n",
    "        for i in range(0, len(rows), embed_batch):\n",
    "            group = rows[i : i + embed_batch]\n",
    "            vectors = embed_texts([g[\"text\"] for g in group])\n",
    "            payload = [{\"chunk_id\": g[\"chunk_id\"], \"embedding\": v} for g, v in zip(group, vectors)]\n",
    "\n",
    "            with driver.session() as s:\n",
    "                s.run(SET_EMBEDDINGS_CYPHER, rows=payload)\n",
    "\n",
    "        print(f\"Done. Ingested book_id={book_id}, pages={total_pages}, chunks={len(chunk_rows)}.\")\n",
    "        return {\n",
    "            \"book_id\": book_id,\n",
    "            \"pages\": total_pages,\n",
    "            \"chunks\": len(chunk_rows),\n",
    "            \"embedded\": len(rows),\n",
    "            \"has_toc_outline\": bool(toc_items),\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "569a2de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 100, model: gemini-embedding-1.0\\nPlease retry in 24.519682647s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-embedding-1.0'}, 'quotaValue': '100'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mGEMINI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mAIzaSyAODasJVvnnhDYAu7-qh9B7jZKbfo-3B7w\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mrun_ingestion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/(The Pearson series in economics) Pindyck, Robert S._ Rubinfeld, Daniel L - Microeconomics-Pearson (2018).pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbook_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpindyck_micro_9e\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMicroeconomics\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauthors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPindyck, Rubinfeld\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43medition\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m9\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2018\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneo4j_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbolt://localhost:7687\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneo4j_user\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneo4j\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneo4j_pass\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtestpassword\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m result\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mrun_ingestion\u001b[39m\u001b[34m(pdf, book_id, title, authors, edition, year, neo4j_uri, neo4j_user, neo4j_pass, embed_model, embed_batch)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rows), embed_batch):\n\u001b[32m    179\u001b[39m     group = rows[i : i + embed_batch]\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     vectors = \u001b[43membed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     payload = [{\u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m: g[\u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m: v} \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(group, vectors)]\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m driver.session() \u001b[38;5;28;01mas\u001b[39;00m s:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mrun_ingestion.<locals>.embed_texts\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_texts\u001b[39m(texts: List[\u001b[38;5;28mstr\u001b[39m]) -> List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Keep 1536 dims so you don't need to change your Neo4j vector index settings\u001b[39;00m\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Gemini defaults to 3072, but output_dimensionality can be set.  [oai_citation:3‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     result = \u001b[43mgemini_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# list batching supported  [oai_citation:4‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\u001b[39;49;00m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbedContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRETRIEVAL_DOCUMENT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# correct for indexing chunks  [oai_citation:5‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\u001b[39;49;00m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_dimensionality\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1536\u001b[39;49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 768/1536/3072 recommended  [oai_citation:6‡Google AI for Developers](https://ai.google.dev/gemini-api/docs/embeddings)\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [e.values \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result.embeddings]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/google/genai/models.py:4167\u001b[39m, in \u001b[36mModels.embed_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4164\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4165\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4167\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4168\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4171\u001b[39m response_dict = {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n\u001b[32m   4173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1388\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1379\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1380\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1383\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1384\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1385\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1386\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1387\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1388\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1389\u001b[39m   response_body = (\n\u001b[32m   1390\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1391\u001b[39m   )\n\u001b[32m   1392\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1224\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1221\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1201\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1194\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1195\u001b[39m       method=http_request.method,\n\u001b[32m   1196\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1199\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1200\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1203\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1204\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/MyApps/econ_visual/venv/lib/python3.13/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 100, model: gemini-embedding-1.0\\nPlease retry in 24.519682647s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/embed_content_free_tier_requests', 'quotaId': 'EmbedContentRequestsPerMinutePerUserPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-embedding-1.0'}, 'quotaValue': '100'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}"
     ]
    }
   ],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAODasJVvnnhDYAu7-qh9B7jZKbfo-3B7w\"\n",
    "\n",
    "result = run_ingestion(\n",
    "    pdf=\"./data/(The Pearson series in economics) Pindyck, Robert S._ Rubinfeld, Daniel L - Microeconomics-Pearson (2018).pdf\",\n",
    "    book_id=\"pindyck_micro_9e\",\n",
    "    title=\"Microeconomics\",\n",
    "    authors=\"Pindyck, Rubinfeld\",\n",
    "    edition=\"9\",\n",
    "    year=\"2018\",\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_pass=\"testpassword\",\n",
    "    embed_model=\"text-embedding-3-small\",\n",
    "    embed_batch=64,\n",
    ")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
